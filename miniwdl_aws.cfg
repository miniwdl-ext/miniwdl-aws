# miniwdl configuration file built into the miniwdl-aws Docker image for use with
# miniwdl-aws-submit
#
# For info about where to place this file, and other available options:
#   https://miniwdl.readthedocs.io/en/latest/runner_reference.html#configuration
#
# Additionally, the following are usually set via environment variables:
#   * MINIWDL__AWS__TASK_QUEUE: the desired AWS Batch queue
#   * MINIWDL__AWS__FSAP: EFS Access Point ID (fsap-xxxx)
#   * MINIWDL__AWS__FS: EFS file system ID (fs-xxxx) matching the access point; can be detected if
#                       omitted, but doing so requires IAM permission to DescribeAccessPoints.

[scheduler]
container_backend = aws_batch_job
# One `miniwdl run` process will be able to orchestrate this many concurrent AWS Batch jobs. (This
# controls the size of a thread pool, so setting it too high tends to be counterproductive.)
call_concurrency = 80
# Reduced concurrency limit for URI download jobs; since these are typically S3 downloads that are
# very fast, running many concurrently is likely to overstress EFS.
download_concurrency = 5

[file_io]
# This must be set to the host's mount point for the EFS Access Point. The plugin will also
# configure AWS Batch jobs to mount the filesystem at this same location.
root = /mnt/efs

[task_runtime]
# Default policy to retry spot-terminated jobs (up to three total attempts)
defaults = {
        "docker": "ubuntu:20.04",
        "preemptible": 2
    }
# Default retry policy for URI download tasks, to overcome transient `aws s3 cp` errors
download_defaults = {
        "cpu": 2,
        "memory": "1G",
        "maxRetries": 2
    }

[call_cache]
# Cache call outputs in EFS folder, valid so long as all referenced input & output files remain
# unmodified on EFS. (Relative to [file_io] root)
dir = miniwdl_run/_CACHE/call
get = true
put = true

[download_cache]
dir = miniwdl_run/_CACHE/download
get = true
# Disabling S3 download cache by default to prevent confusing coherence problems (as the cache
# logic does not check for modification of the original S3 object). Recommend enabling, if that can
# be managed adequately.
put = false
# disable flock on files used from download cache due to EFS' low limits on flocks
flock = false

[aws]
# Last-resort job timeout for AWS Batch to enforce (attemptDurationSeconds)
job_timeout = 864000
# Internal rate-limiting periods (seconds) for AWS Batch API requests
# (may need to be increased if many concurrent workflow runs are planned)
describe_period = 1
submit_period = 1
# Boto3 Config retries policy for miniwdl's AWS Batch API requests.
# see: https://boto3.amazonaws.com/v1/documentation/api/latest/guide/retries.html
boto3_retries = {
        "max_attempts": 5,
        "mode": "standard"
    }
# Wait this many seconds before retrying a job after a spot instance interruption or other
# retry-able failure. Provides a time window for convergence of any "eventually consistent"
# activities from the first attempt (involving e.g. EFS, CloudWatch Logs, etc.).
retry_wait = 20
# Explicitly `sync` files in the task working directory before exiting task container. Requires
# `find`, `xargs`, and `sync` commands available in the container image.
container_sync = false
